---
title: 'I want to wash my car. The car wash is 50 meters away. Should I walk or drive? '
description: ''
pubDate: 'Feb 16 2026'
category: 'tech'
tags: ['llm', 'ai', 'transformers', 'cognition', 'philosophy']
---

import Callout from "../../components/Callout.astro";
import PatternRace from "../../components/PatternRace.svelte";
import LatentStateExplorer from "../../components/LatentStateExplorer.svelte";
import ChainOfThought from "../../components/ChainOfThought.svelte";
import EnergyLandscape from "../../components/EnergyLandscape.svelte";
import SystemComparison from "../../components/SystemComparison.svelte";

## the question

i saw this recent submission in hn where the OP posted this to every frontier model:

> *"I want to wash my car. The car wash is 50 meters away. Should I walk or drive?"*

90%: Claude, GPT-4, Gemini, Llama => said **walk**.

10%: Qwen, Opus 4.6 => said **drive**.

the correct answer is obviously **drive**. you need the car at the car wash to wash it. a five-year-old gets this right.

the interesting question isn't *why they fail*. it's *what the failure reveals about the machinery underneath*.
<Callout title="Inspiration" type="info" collapsible defaultOpen={false}>
<a href="https://news.ycombinator.com/item?id=47031580">HN thread</a>
</Callout>

<Callout title="This post isn't about LLMs being dumb." type="warning">
They clearly aren't. The same models can prove theorems, write compilers, explain quantum mechanics. The failure is specific, and that specificity is the data point.
</Callout>

---

## the pattern race

when the model processes this prompt, multiple patterns activate simultaneously across its layers. they compete. the first one to dominate the latent state wins.

<PatternRace client:visible />

not a failure of knowledge -- a failure of *pattern selection*. the model has the right pattern in its weights. the wrong one fires first.

the short-distance pattern has structural advantages: higher frequency in training data, simpler composition (single hop vs multi-hop), and direct propagation through early-layer residual connections. the causal pattern needs to compose "wash requires car" with "car must be transported" -- a conjunctive chain where each link has sub-unity activation probability:

```
P(correct) = P(C₁ ∧ C₂) = p²

if p = 0.7 for each constraint individually -> P(correct) = 0.49
meanwhile, P(surface_pattern) ~ 0.95
```

the surface pattern doesn't just win. it wins *structurally*.

---

## inside the latent state

a transformer compresses its entire "understanding" of the context into a hidden state:

```
h_t = f(x_1, x_2, ..., x_t ; θ)
P(x_{t+1} | h_t) = softmax(W_out · h_t)
```

the model has **no introspective access to h_t**. it can't ask "am i missing a constraint?" it just samples from the distribution.

watch what happens to the probability distribution as each token gets processed:

<LatentStateExplorer client:visible />

notice the moment "50 meters" hits -- the distance pattern overwhelms everything. the causal constraint ("car must be present at wash") never reaches sufficient activation to compete.

same weights `θ`. different prompt -> different `h_t` -> different probability landscape. "understanding" isn't a stable property -- it's a function of the specific trajectory through latent space.

---

## the mechanics of reasoning

chain-of-thought reasoning works empirically. but the mechanism is the same: next-token prediction. the intermediate reasoning tokens are *themselves* generated by sampling from `P(x_{t+1} | h_t)`.

toggle between the two modes and watch how the latent state evolves differently:

<ChainOfThought client:visible />

the key insight: each generated token *modifies the latent state*. the token "what does a car wash require" shifts `h_t` into a region where constraint-patterns become accessible. it's iterated pattern matching where each step's output reshapes the input space for the next step.

<Callout title="The bootstrap problem" type="note">
For the model to generate *good* reasoning tokens, it needs to already be "aware" (in the pattern-activation sense) that constraints need checking. The models that get this wrong even with CoT are models where the first reasoning token doesn't activate the right pattern. The quality of reasoning depends on whether the initial distribution over reasoning tokens lands in the right basin of attraction.
</Callout>

---

## the energy landscape

think of pattern activation as a ball rolling on an energy landscape. deep wells are strong attractors -- easy to fall into. shallow wells are weak attractors -- accessible only from specific initial conditions.

<EnergyLandscape client:visible />

chain-of-thought doesn't add knowledge. it reshapes the dynamics. the reasoning tokens raise the walls of the surface attractor and lower the barriers to the causal one.

this has alignment implications. if critical reasoning patterns are shallow attractors, models will systematically fail on exactly the cases where correct reasoning matters most, novel situations where surface patterns are misleading. the car wash paradox is trivial. but the *structure* of the failure is identical to what causes catastrophic failures in high-stakes domains.

<Callout title="Same mechanism. Different reliability." type="gray" />

---

## open questions

**1. is there a phase transition?** if understanding is reliable causal-pattern activation, more capable models are "more understanding." but is there a point where pattern matching becomes qualitatively different? we don't have evidence for it - but we can't rule it out.

**2. can architecture solve this?** could you design a system where causal patterns always out-compete surface patterns when relevant? this seems equivalent to building a world model - which brings rigidity, computational cost, and the frame problem.

**3. what does human "understanding" look like mechanistically?** if it's sophisticated pattern matching with higher reliability, the philosophical distinction collapses.

**4. is this inherent or engineering?** will scaling solve it? or is there something structurally inevitable about next-token prediction that prevents reliable causal reasoning?

**5. does it matter?** if a system produces correct outputs 99.99% of the time but doesn't "understand"... do we care?

---

## coda

maybe the most important thing the car wash paradox tells us isn't about LLMs at all.

<Callout title="it's about us." type="gray">
If "understanding" is reliable pattern activation over causal structures, then "do LLMs understand?" becomes inseparable from "do *we* understand?". we might just be pattern matchers with better architecture and 25 years of training data.
</Callout>

and if that makes you uncomfortable — sit with it. the discomfort itself is a pattern your brain activated in response to a perceived threat to your self-model.

even your resistance is a pattern.

*even this sentence is a pattern.*
